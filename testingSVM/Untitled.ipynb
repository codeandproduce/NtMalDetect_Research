{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "useTFIDF = True\n",
    "showSampleVector = False\n",
    "showMostInformativeFeatures = True\n",
    "howManyInformativeFeatures = 20\n",
    "nGRAM1 = 8\n",
    "nGRAM2 = 10\n",
    "weight = 4\n",
    "\n",
    "ask = input(\"Do you want to specify parameters or use default values? Input 'T' or 'F'.   \")\n",
    "if ask == \"T\":\n",
    "    useTFIDFStr = input(\"Do you want to use tfidfVectorizer or CountVectorizer? Type T for tfidfVectorizer and F for CountVectorizer   \")\n",
    "    if useTFIDFStr == \"T\":\n",
    "        useTFIDF = True\n",
    "    else:\n",
    "        useTFIDF = False\n",
    "\n",
    "    showSampleVectorStr = input(\"Do you want to print an example vectorized corpus? (T/F)   \")\n",
    "    if showSampleVectorStr == \"T\":\n",
    "        showSampleVector = True\n",
    "    else:\n",
    "        showSampleVector = False\n",
    "\n",
    "    showMostInformativeFeaturesStr = input(\"Do you want to print the most informative feature in some of the classifiers? (T/F)   \")\n",
    "    if showMostInformativeFeaturesStr == \"T\":\n",
    "        showMostInformativeFeatures = True\n",
    "        howManyInformativeFeatures = int(input(\"How many of these informative features do you want to print for each binary case? Input a number   \"))\n",
    "    else:\n",
    "        showMostInformativeFeatures = False\n",
    "\n",
    "    nGRAM1 = int(input(\"N-Gram lower bound (Read README.md for more information)? Input a number   \"))\n",
    "    nGRAM2 = int(input(\"N-Gram Upper bound? Input a number   \"))\n",
    "    weight = int(input(\"What weight do you want to use to separate train & testing? Input a number   \"))\n",
    "\n",
    "\n",
    "main_corpus = []\n",
    "main_corpus_target = []\n",
    "\n",
    "my_categories = ['benign', 'malware']\n",
    "\n",
    "# feeding corpus the testing data\n",
    "\n",
    "print(\"Loading system call database for categories:\")\n",
    "print(my_categories if my_categories else \"all\")\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "malCOUNT = 0\n",
    "benCOUNT = 0\n",
    "for filename in glob.glob(os.path.join('./sysMAL', '*.txt')):\n",
    "    fMAL = open(filename, \"r\")\n",
    "    aggregate = \"\"\n",
    "    for line in fMAL:\n",
    "        linea = line[:(len(line)-1)]\n",
    "        aggregate += \" \" + linea\n",
    "    main_corpus.append(aggregate)\n",
    "    main_corpus_target.append(1)\n",
    "    malCOUNT += 1\n",
    "\n",
    "for filename in glob.glob(os.path.join('./sysBEN', '*.txt')):\n",
    "    fBEN = open(filename, \"r\")\n",
    "    aggregate = \"\"\n",
    "    for line in fBEN:\n",
    "        linea = line[:(len(line) - 1)]\n",
    "        aggregate += \" \" + linea\n",
    "    main_corpus.append(aggregate)\n",
    "    main_corpus_target.append(0)\n",
    "    benCOUNT += 1\n",
    "\n",
    "# shuffling the dataset\n",
    "main_corpus_target, main_corpus = shuffle(main_corpus_target, main_corpus, random_state=0)\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "# weight as determined in the top of the code\n",
    "train_corpus = main_corpus[:(weight*len(main_corpus)//(weight+1))]\n",
    "train_corpus_target = main_corpus_target[:(weight*len(main_corpus)//(weight+1))]\n",
    "test_corpus = main_corpus[(len(main_corpus)-(len(main_corpus)//(weight+1))):]\n",
    "test_corpus_target = main_corpus_target[(len(main_corpus)-len(main_corpus)//(weight+1)):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# size of datasets\n",
    "train_corpus_size_mb = size_mb(train_corpus)\n",
    "test_corpus_size_mb = size_mb(test_corpus)\n",
    "\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(train_corpus_target), train_corpus_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(test_corpus_target), test_corpus_size_mb))\n",
    "print(\"%d categories\" % len(my_categories))\n",
    "print()\n",
    "print(\"Benign Traces: \"+str(benCOUNT)+\" traces\")\n",
    "print(\"Malicious Traces: \"+str(malCOUNT)+\" traces\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer...\")\n",
    "t0 = time()\n",
    "\n",
    "if useTFIDF:\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(nGRAM1, nGRAM2), min_df=1, use_idf=True, smooth_idf=True) ##############\n",
    "else:\n",
    "    vectorizer = CountVectorizer(ngram_range=(nGRAM1, nGRAM2))\n",
    "\n",
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_corpus)\n",
    "\n",
    "if showSampleVector:\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X_train.toarray()[0])\n",
    "\n",
    "\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, train_corpus_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer...\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(test_corpus)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, test_corpus_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# show which are the definitive features\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    coefs_with_fns_mal = coefs_with_fns[:-(n + 1):-1]\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))[:n]\n",
    "\n",
    "    print()\n",
    "    print(\"Most Informative Benign Features:\")\n",
    "    for (coef_1, fn_1) in coefs_with_fns:\n",
    "        print(coef_1, fn_1)\n",
    "    print()\n",
    "    print(\"Most Informative Malicious Features:\")\n",
    "    for (coef_2, fn_2) in coefs_with_fns_mal:\n",
    "        print(coef_2, fn_2)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "test_corpus_target = np.array(test_corpus_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffling the dataset\n",
    "main_corpus_target, main_corpus = shuffle(main_corpus_target, main_corpus, random_state=2)\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "# weight as determined in the top of the code\n",
    "train_corpus = main_corpus[:(weight*len(main_corpus)//(weight+1))]\n",
    "train_corpus_target = main_corpus_target[:(weight*len(main_corpus)//(weight+1))]\n",
    "test_corpus = main_corpus[(len(main_corpus)-(len(main_corpus)//(weight+1))):]\n",
    "test_corpus_target = main_corpus_target[(len(main_corpus)-len(main_corpus)//(weight+1)):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "Training: \n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=0, shrinking=True, tol=0.001,\n",
      "  verbose=False)\n",
      "0.613636363636\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=0)\n",
    "print('_'*60)\n",
    "print(\"Training: \")\n",
    "print(classifier)\n",
    "probas_ = classifier.fit(X_train, train_corpus_target).predict_proba(X_test)\n",
    "\n",
    "# print(\"shape test_corpus_target: \", np.shape(test_corpus_target))\n",
    "# print(\"shape probas: \", np.shape(probas_[:, 0]))\n",
    "\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "score = metrics.accuracy_score(test_corpus_target, pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_corpus_target:\", np.array(test_corpus_target))\n",
    "print(\"probas: \", probas_[:, 1])\n",
    "\n",
    "for i in range(len(probas_)):\n",
    "    print(\"target:\", test_corpus_target[i])\n",
    "    print(\"proba: \", probas_[i,1])\n",
    "    print(\"difference\", abs(test_corpus_target[i]-probas_[i,1]))\n",
    "    print(\"===========\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(np.array(test_corpus_target), probas_[:, 0], pos_label=1, drop_intermediate=False)\n",
    "print(\"thresholds: \", _)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"fpr\", fpr)\n",
    "print(\"tpr\", tpr)\n",
    "print(\"roc\", roc_auc)\n",
    "\n",
    "for i in range(len(fpr)):\n",
    "    print(i)\n",
    "    print(\"fpr\", fpr[i])\n",
    "    print(\"tpr\", tpr[i])\n",
    "    print(\"threshold\", _[i])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# printing ROC\n",
    "plt.figure()\n",
    "lwr = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lwr, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
